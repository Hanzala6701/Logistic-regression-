{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQzRnwm1yObk2bKi7UtlTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanzala6701/Logistic-regression-/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "- Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (0 or 1, true/false) based on input data using a sigmoid function. It differs from Linear Regression, which predicts continuous, numerical values (e.g., house prices) using a straight line. Logistic regression outputs a probability (0-1 range), whereas Linear regression predicts unrestricted values.\n",
        "Key Differences Between Logistic and Linear Regression\n",
        "\n",
        "i) Purpose: Logistic regression solves classification problems (yes/no, spam/not-spam), while linear regression handles regression problems (predicting quantities).\n",
        "\n",
        "ii) Output Type: Logistic predicts a categorical, discrete value (0 or 1), while Linear predicts a continuous, numerical value.\n",
        "\n",
        "iii) Function/Curve: Logistic uses the S-shaped sigmoid (logistic) function to map predictions to a 0-1 range. Linear uses a straight line."
      ],
      "metadata": {
        "id": "cVAqJAFk_4j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "-  The sigmoid function in logistic regression transforms the model's linear output (any real number) into a probability (a value between 0 and 1) by squashing it into an \"S\"-shaped curve, allowing for binary classification by setting a threshold (usually 0.5) to decide between class 0 and class 1, effectively modeling the likelihood of an event.\n",
        "\n",
        "#Key Roles of the Sigmoid Function:\n",
        "\n",
        "i) Probability Mapping: It takes the weighted sum of inputs (a linear equation, \\(z=w^{T}x+b\\)) and maps it to a value between 0 and 1, which is perfect for representing the probability \\(P(y=1|x)\\).\n",
        "\n",
        "ii) Binary Classification: Since probabilities are between 0 and 1, the output can be interpreted as the chance of belonging to the positive class. A common threshold of 0.5 is used: if output \\(\\ge \\) 0.5, predict class 1; if \\(<\\) 0.5, predict class 0.\n",
        "\n",
        "iii) S-Shaped Curve: Its \"S\" shape (logistic curve) ensures that as the input \\(z\\) goes to positive infinity, the output approaches 1, and as \\(z\\) goes to negative infinity, the output approaches 0, preventing probabilities from exceeding valid ranges.\n",
        "\n",
        "iv) Differentiability: The sigmoid function has a simple derivative, which is crucial for training the model using gradient descent to adjust weights."
      ],
      "metadata": {
        "id": "vN1fR7FsAJES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) What is Regularization in Logistic Rea technique used to prevent overfitting by  a penalty term to the cost function, which discourages the model from learning excessively complex patterns or assigning large weights to featuresgression and why is it needed?\n",
        "\n",
        "- Regularization in logistic regression is . It forces the model to generalize better to new data by shrinking parameter coefficients.\n",
        "\n",
        "# Why Regularization is Needed:\n",
        "\n",
        "\n",
        "i) Prevents Overfitting: Without regularization, logistic regression might fit the training data too closely, capturing noise instead of the underlying pattern, leading to high variance and poor performance on test data.\n",
        "\n",
        "ii) Handles High-Dimensional Data: When the number of features is high or there is multicollinearity, coefficients can become extremely large. Regularization keeps these weights small.\n",
        "\n",
        "iii) Improves Model Generalization: It encourages simpler models (Occam's Razor), balancing training accuracy with better performance on unseen data.\n",
        "\n",
        "iv) Feature Selection (Lasso): L1 regularization can drive coefficients to exactly zero, effectively performing feature selection."
      ],
      "metadata": {
        "id": "M8o-qvGmArir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        " -  Common classification evaluation metrics include Accuracy, Precision, Recall, F1-Score, and AUC-ROC, which are crucial for assessing model performance, detecting overfitting, and handling class imbalances. These metrics determine if a model generalizes well and meets specific business goals (e.g., minimizing false negatives in medical diagnostics).\n",
        "\n",
        "i) Accuracy: The ratio of correct predictions to total predictions, useful for balanced datasets but misleading for imbalanced data.\n",
        "\n",
        "ii) Precision: Measures the accuracy of positive predictions (True Positives / All Predicted Positives); critical when false positives are expensive (e.g., email spam detection).\n",
        "\n",
        "iii) Recall (Sensitivity): Measures the ability to find all positive instances (True Positives / All Actual Positives); crucial when false negatives are costly (e.g., cancer detection).\n",
        "\n",
        "iv) F1-Score: The harmonic mean of precision and recall, providing a balance between the two, especially for imbalanced data.\n",
        "\n",
        "v) AUC-ROC: Area Under the Receiver Operating Characteristic curve, representing the model's ability to distinguish between classes across different thresholds."
      ],
      "metadata": {
        "id": "nUbN4Cg5B2aJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49aTEDuB5TIJ",
        "outputId": "6950c733-cd3e-4cb5-963f-aa1812badb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame head (first 5 rows):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "\n",
            "Training set size: 105 samples\n",
            "Test set size: 45 samples\n",
            "\n",
            "Logistic Regression model trained successfully.\n",
            "Accuracy of the model on the test set: 0.9778\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#5) Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "def run_logistic_regression():\n",
        "    # 1. Load a sample dataset from scikit-learn\n",
        "    # In a real scenario, you would use pd.read_csv('your_file.csv')\n",
        "    iris = load_iris(as_frame=True)\n",
        "    X = iris.data # Features\n",
        "    y = iris.target # Target variable\n",
        "\n",
        "    # Convert to a single Pandas DataFrame for demonstration purposes as requested\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "    print(\"DataFrame head (first 5 rows):\")\n",
        "    print(df.head())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 2. Split into train/test sets\n",
        "    # test_size=0.3 means 30% of the data is used for testing\n",
        "    # random_state ensures reproducibility of the split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training set size: {len(X_train)} samples\")\n",
        "    print(f\"Test set size: {len(X_test)} samples\\n\")\n",
        "\n",
        "    # 3. Train a Logistic Regression model\n",
        "    # Instantiate the model\n",
        "    model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "    # Fit the model with the training data\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Logistic Regression model trained successfully.\")\n",
        "\n",
        "    # 4. Make predictions and print the accuracy\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy of the model on the test set: {accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_logistic_regression()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6)Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_logistic_regression_with_l2():\n",
        "    \"\"\"\n",
        "    Trains a Logistic Regression model with L2 regularization (Ridge)\n",
        "    using a dataset from scikit-learn, prints the coefficients and accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Load a sample dataset (e.g., Iris dataset)\n",
        "    # This is a standard classification dataset provided by scikit-learn.\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    print(f\"Dataset loaded: {iris.DESCR.splitlines()[0]}\")\n",
        "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\\n\")\n",
        "\n",
        "    # 2. Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing set size: {X_test.shape[0]} samples\\n\")\n",
        "\n",
        "    # 3. Initialize the Logistic Regression model with L2 regularization\n",
        "    # The 'penalty' parameter is set to 'l2' by default.\n",
        "    # The 'C' parameter controls the inverse of regularization strength.\n",
        "    # Smaller values of C mean stronger regularization.\n",
        "    # We use a moderate C value (1.0) here.\n",
        "    model = LogisticRegression(\n",
        "        penalty='l2',        # Use L2 regularization\n",
        "        C=1.0,               # Inverse of regularization strength\n",
        "        solver='liblinear',  # Good for small datasets\n",
        "        random_state=42,\n",
        "        max_iter=1000        # Increase iterations for convergence if needed\n",
        "    )\n",
        "\n",
        "    print(\"Model initialized with L2 regularization (penalty='l2')...\")\n",
        "\n",
        "    # 4. Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\\n\")\n",
        "\n",
        "    # 5. Evaluate the model on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # 6. Print the model coefficients and intercepts\n",
        "    # For multi-class (as in Iris), coefficients are per class.\n",
        "    print(\"\\nModel Coefficients (Weights) per Class:\")\n",
        "    # Using np.around for cleaner output\n",
        "    for i, class_coeffs in enumerate(model.coef_):\n",
        "        print(f\"  Class {i}: {np.around(class_coeffs, 4)}\")\n",
        "\n",
        "    print(\"\\nModel Intercepts (Bias) per Class:\")\n",
        "    print(np.around(model.intercept_, 4))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_logistic_regression_with_l2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgzAYnnB6A-6",
        "outputId": "15c450c2-d421-4989-9b86-d9ddf51222bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: .. _iris_dataset:\n",
            "Features shape: (150, 4), Target shape: (150,)\n",
            "\n",
            "Training set size: 105 samples\n",
            "Testing set size: 45 samples\n",
            "\n",
            "Model initialized with L2 regularization (penalty='l2')...\n",
            "Model training complete.\n",
            "\n",
            "----------------------------------------\n",
            "Model Accuracy on Test Set: 0.9778\n",
            "----------------------------------------\n",
            "\n",
            "Model Coefficients (Weights) per Class:\n",
            "  Class 0: [ 0.3648  1.355  -2.0963 -0.9215]\n",
            "  Class 1: [ 0.4809 -1.5846  0.3938 -1.0922]\n",
            "  Class 2: [-1.5286 -1.4324  2.3048  2.0858]\n",
            "\n",
            "Model Intercepts (Bias) per Class:\n",
            "[ 0.2363  1.0382 -1.0465]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7) Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr and print the classification report. (Use Dataset from sklearn package)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the dataset (Iris dataset is commonly used for multiclass examples)\n",
        "# Docs at https://scikit-learn.org\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize the Logistic Regression model with multi_class='ovr' (One-vs-Rest)\n",
        "# Setting solver='liblinear' is a good choice for smaller datasets and ovr scheme.\n",
        "# More details on parameters can be found in the scikit-learn [LogisticRegression documentation](https://scikit-learn.org).\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=100)\n",
        "\n",
        "# 4. Train the model on the training data\n",
        "print(\"Training Logistic Regression model with multi_class='ovr'...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Print the classification report\n",
        "# The report provides precision, recall, f1-score, and support for each class.\n",
        "# Read more in the scikit-learn [classification_report documentation](https://scikit-learn.org).\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nModel details:\")\n",
        "print(f\"Classes: {model.classes_}\")\n",
        "print(f\"Number of iterations to converge: {model.n_iter_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLmFc_Z464Qe",
        "outputId": "fbc949a6-8a65-4b83-ab49-e142d6c7f068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model with multi_class='ovr'...\n",
            "Training complete.\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      0.92      0.96        13\n",
            "   virginica       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "\n",
            "Model details:\n",
            "Classes: [0 1 2]\n",
            "Number of iterations to converge: [7 6 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"#8) Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "\n",
        "(Use Dataset from sklearn package)\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load and prepare the dataset\n",
        "# We will use the breast cancer dataset for this example.\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets (optional, but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features (important for L2 penalty and for consistent C values)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 2. Define the Logistic Regression model\n",
        "# It's important to set 'solver' and 'max_iter' for compatibility with various penalties.\n",
        "# 'liblinear' works well for both 'l1' and 'l2' penalties.\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "# 3. Define the parameter grid for GridSearchCV\n",
        "# C is the inverse of regularization strength; smaller values mean stronger regularization.\n",
        "# penalty options are 'l1' (Lasso) and 'l2' (Ridge).\n",
        "param_grid = {\n",
        "    'C': np.logspace(-3, 3, 7),  # e.g., 0.001, 0.01, 0.1, 1, 10, 100, 1000\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# 4. Initialize and run GridSearchCV\n",
        "# 'cv' specifies the number of cross-validation folds.\n",
        "# 'scoring' defines the metric to optimize (default is accuracy for classification).\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=0,\n",
        "    n_jobs=-1  # Use all available processors\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Print the best parameters and validation accuracy\n",
        "print(f\"Best parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Optional: Evaluate the best model on the held-out test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(f\"Accuracy on the held-out test set: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdWXOlCG7Nmg",
        "outputId": "a2f5262a-f225-4c1d-94c9-14cc70301376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV: {'C': np.float64(0.1), 'penalty': 'l2'}\n",
            "Best cross-validation accuracy: 0.9774\n",
            "Accuracy on the held-out test set: 0.9942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"9)Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "(Use Dataset from sklearn package)\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "# We use a fixed random state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model without Feature Scaling ---\n",
        "# 3. Train the Logistic Regression model on unscaled data\n",
        "model_no_scale = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear') # increased max_iter for convergence\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and evaluate accuracy\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "\n",
        "# --- Model with Feature Scaling (Standardization) ---\n",
        "# 5. Create a pipeline that scales features and then trains the model\n",
        "# Using a pipeline prevents data leakage\n",
        "model_with_scale = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'))\n",
        "\n",
        "# 6. Train the model on the scaled data (pipeline handles scaling internally)\n",
        "model_with_scale.fit(X_train, y_train)\n",
        "\n",
        "# 7. Predict and evaluate accuracy\n",
        "y_pred_with_scale = model_with_scale.predict(X_test)\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_with_scale)\n",
        "print(f\"Accuracy with scaling:   {accuracy_with_scale:.4f}\")\n",
        "\n",
        "# 8. Comparison of results\n",
        "print(\"\\nComparison of Accuracies:\")\n",
        "print(f\"Difference: {abs(accuracy_with_scale - accuracy_no_scale):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWH9-4Bm7_iD",
        "outputId": "5ecc6119-2075-4aa3-bc57-36a120579f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9649\n",
            "Accuracy with scaling:   0.9825\n",
            "\n",
            "Comparison of Accuracies:\n",
            "Difference: 0.0175\n"
          ]
        }
      ]
    }
  ]
}